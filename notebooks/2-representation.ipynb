{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "['test_samples_1.csv', 'test_samples_2.csv', 'train_samples_1.csv', 'train_samples_2.csv']\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "save = \"../data/for_step_2/\"\n",
    "\n",
    "print(os.path.exists(save))\n",
    "print(os.listdir(save))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading our prepped Yelp dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We're now trying to get an output from *mistral 7B* that we can fine-tune *phi-2* with.\n",
    "> - Read more about how this came about in the decision making process in the [project journal](../Project_Journal.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (9999, 5) (4998, 5)\n",
      "Test: (1998, 5) (999, 5)\n"
     ]
    }
   ],
   "source": [
    "# Load train and test data\n",
    "train10k = pd.read_csv(save + 'train_samples_1.csv')\n",
    "train5k = pd.read_csv(save + 'train_samples_2.csv')\n",
    "test2k = pd.read_csv(save + 'test_samples_1.csv')\n",
    "test1k = pd.read_csv(save + 'test_samples_2.csv')\n",
    "\n",
    "print('Train:', train10k.shape, train5k.shape)\n",
    "print('Test:', test2k.shape, test1k.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up `Mistral-7B-Instruct-v0.3` for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8edd5db6c4ae497f80dff51590fe8409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32768, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define 4-bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Load in 4-bit to reduce VRAM usage\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # Use float16 for computations\n",
    "    bnb_4bit_use_double_quant=True,  # Double quantization for further memory reduction\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Use NF4 quantization (optimized for LLMs)\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Get the model's device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.eval()  # Set to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: 0 - NVIDIA GeForce RTX 3060\n",
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA Available: True\n",
      "CUDA device count: 1\n",
      "CUDA device name: NVIDIA GeForce RTX 3060\n",
      "GPU memory allocated: 4138.59 MB\n",
      "GPU memory cached: 4160.75 MB\n"
     ]
    }
   ],
   "source": [
    "# Print the current GPU being used\n",
    "current_device = torch.cuda.current_device()\n",
    "print(f\"Using GPU: {current_device} - {torch.cuda.get_device_name(current_device)}\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Check your installation.\")\n",
    "\n",
    "# Print GPU memory usage\n",
    "print(f\"GPU memory allocated: {torch.cuda.memory_allocated()/1e6:.2f} MB\")\n",
    "print(f\"GPU memory cached: {torch.cuda.memory_reserved()/1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain why customer feedback is important for improving business processes.  Customer feedback is crucial for improving business processes for several reasons:\n",
      "\n",
      "1. Identifying Strengths and Weaknesses: Feedback provides insights into what the business is doing well and where it needs improvement. This information can help businesses focus on their strengths and address areas that need improvement.\n",
      "\n",
      "2. Understanding Customer Needs: Feedback helps businesses understand their customers' needs, expectations, and preferences. This understanding can guide product development, service improvements, and marketing strategies.\n",
      "\n",
      "3. Enhancing Customer Satisfaction: By addressing issues raised in feedback, businesses can improve their products or services, leading to increased customer satisfaction. Satisfied customers are more likely to remain loyal and recommend the business to others\n"
     ]
    }
   ],
   "source": [
    "# Prepare input text\n",
    "text_test = \"[INST] Explain why customer feedback is important for improving business processes. [/INST]\"\n",
    "inputs = tokenizer(text_test, return_tensors=\"pt\").to(device)  # Move input tensors to the model's device\n",
    "\n",
    "# Generate output\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, \n",
    "                             max_new_tokens=150,\n",
    "                             pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# Decode and print the response\n",
    "print(tokenizer.decode(outputs[0], \n",
    "                       skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "def create_prompt(text, emotions, label):\n",
    "    text =  f\"A customer left us a {label}-star review: '{text}' The customer feels {emotions}. Concisely, how can we best improve our services for this customer's experience?\"\n",
    "    return f\"[INST] {text} [/INST]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] A customer left us a 2-star review: 'totally dont uunderstand this place........at all....! n nare you a mexican restaurant....are you a pub......what are you? the food is ok the wine is questionable for the price ther service is ok. n nits just a bit pointless great location going to waste in my opinion.' The customer feels disapproval, annoyance, confusion. Concisely, how can we best improve our services for this customer's experience? [/INST]\n"
     ]
    }
   ],
   "source": [
    "# Test inference\n",
    "sample_text = train10k['clean_text'][11]\n",
    "sample_emotions = train10k['emotions'][11]\n",
    "sample_stars = train10k['label'][11]\n",
    "\n",
    "prompt = create_prompt(sample_text, sample_emotions, sample_stars)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Response (without prompt):\n",
      "To improve the customer's experience, we can take the following steps:\n",
      "\n",
      "1. Clarify our establishment's identity: Ensure that our online presence clearly communicates our restaurant's concept, cuisine, and atmosphere to avoid confusion.\n",
      "\n",
      "2. Enhance menu offerings: Review our menu to ensure a consistent quality of food and competitive pricing, especially for wine.\n",
      "\n",
      "3. Improve service: Train our staff to provide friendly, attentive, and efficient service that meets customer expectations.\n",
      "\n",
      "4. Address cleanliness: Ensure the restaurant is clean and free of any distractions, such as nits, to create a more pleasant dining experience.\n",
      "\n",
      "5. Engage with the customer: Respond to the review, acknowledging their feedback and expressing our commitment to improving their experience in the future.\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the input prompt and move to the model's device\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate output using the model\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs,\n",
    "                             max_new_tokens=180,\n",
    "                             pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# Decode the output and remove the prompt part from the response\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "response = generated_text\n",
    "\n",
    "def clean_response(text):\n",
    "    import re\n",
    "    \n",
    "    # Find last period\n",
    "    last_period = text.rfind('.')\n",
    "    if last_period == -1:\n",
    "        return text\n",
    "    \n",
    "    # Get text up to last period\n",
    "    cleaned = text[:last_period+1]\n",
    "    \n",
    "    # Check for numbered list pattern at the end (like \"5.\" with nothing after it)\n",
    "    pattern = r'\\n\\d+\\.\\s*$'\n",
    "    cleaned = re.sub(pattern, '', cleaned)\n",
    "    \n",
    "    return cleaned.strip()\n",
    "\n",
    "response = clean_response(response)\n",
    "print(\"\\nGenerated Response (without prompt):\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prompt # Redefining here for clarity, but same as above.\n",
    "def create_prompt(text, emotions, label):\n",
    "    text =  f\"A customer left us a {label}-star review: '{text}' The customer feels {emotions}. Concisely, how can we best improve our services for this customer's experience?\"\n",
    "    return f\"[INST] {text} [/INST]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train10k' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Apply to all samples\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train10k[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtrain10k\u001b[49m\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: create_prompt(x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_text\u001b[39m\u001b[38;5;124m'\u001b[39m], x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memotions\u001b[39m\u001b[38;5;124m'\u001b[39m], x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      3\u001b[0m train5k[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m train5k\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: create_prompt(x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_text\u001b[39m\u001b[38;5;124m'\u001b[39m], x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memotions\u001b[39m\u001b[38;5;124m'\u001b[39m], x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m test2k[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m test2k\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: create_prompt(x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_text\u001b[39m\u001b[38;5;124m'\u001b[39m], x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memotions\u001b[39m\u001b[38;5;124m'\u001b[39m], x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train10k' is not defined"
     ]
    }
   ],
   "source": [
    "# Apply to all samples\n",
    "train10k['prompt'] = train10k.apply(lambda x: create_prompt(x['clean_text'], x['emotions'], x['label']), axis=1)\n",
    "train5k['prompt'] = train5k.apply(lambda x: create_prompt(x['clean_text'], x['emotions'], x['label']), axis=1)\n",
    "test2k['prompt'] = test2k.apply(lambda x: create_prompt(x['clean_text'], x['emotions'], x['label']), axis=1)\n",
    "test1k['prompt'] = test1k.apply(lambda x: create_prompt(x['clean_text'], x['emotions'], x['label']), axis=1)\n",
    "\n",
    "# Print shapes\n",
    "print('Train:', train10k.shape, train5k.shape)\n",
    "print('Test:', test2k.shape, test1k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_response(text):\n",
    "    import re\n",
    "    \n",
    "    # Remove any occurrences of '</s>' (special token for end of sequence)\n",
    "    text = text.replace(\"</s>\", \"\").strip()\n",
    "    \n",
    "    # Find last period\n",
    "    last_period = text.rfind('.')\n",
    "    if last_period == -1:\n",
    "        return text\n",
    "    \n",
    "    # Get text up to last period\n",
    "    cleaned = text[:last_period + 1]\n",
    "    \n",
    "    # Check for numbered list pattern at the end (like \"5.\" with nothing after it)\n",
    "    pattern = r'\\n\\d+\\.\\s*$'\n",
    "    cleaned = re.sub(pattern, '', cleaned)\n",
    "    \n",
    "    return cleaned.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call model once\n",
    "def generate_1response(input_col):\n",
    "    # Create prompt\n",
    "    prompt = input_col\n",
    "    \n",
    "    # Tokenize the input prompt and move to the model's device\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate output using the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs,\n",
    "                                 max_new_tokens=150,\n",
    "                                 pad_token_id=tokenizer.eos_token_id\n",
    "                                 )\n",
    "\n",
    "    # Decode the output and remove the prompt part from the response\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Clean response\n",
    "    response = clean_response(generated_text)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To improve the customer's experience, we can take the following steps:\n",
      "\n",
      "1. Clarify our establishment's identity: Ensure that our online presence clearly communicates our restaurant's concept, cuisine, and atmosphere to avoid confusion.\n",
      "\n",
      "2. Enhance menu offerings: Review our menu to ensure a consistent quality of food and competitive pricing, especially for wine.\n",
      "\n",
      "3. Improve service: Train our staff to provide friendly, attentive, and efficient service that meets customer expectations.\n",
      "\n",
      "4. Address cleanliness: Ensure the restaurant is clean and free of any distractions, such as nits, to create a more pleasant dining experience.\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "print(generate_1response(train10k['prompt'][11]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import pickle\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to generate responses\n",
    "def generate_response(prompts, batch_size=8, checkpoint_path='inference_checkpoint.pkl'):\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    all_responses = [\"\"] * len(prompts)  # Pre-allocate list\n",
    "    start_batch = 0\n",
    "    prompts = list(prompts)\n",
    "\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        try:\n",
    "            with open(checkpoint_path, 'rb') as f:\n",
    "                checkpoint = pickle.load(f)\n",
    "                all_responses = checkpoint['responses']\n",
    "                start_batch = checkpoint['next_batch']\n",
    "                print(f\"Resuming from checkpoint: {start_batch * batch_size}/{len(prompts)} items processed\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading checkpoint: {e}\")\n",
    "            all_responses = [\"\"] * len(prompts)\n",
    "            start_batch = 0\n",
    "\n",
    "    try:\n",
    "        for i in tqdm(range(start_batch * batch_size, len(prompts), batch_size), desc=\"Processing batches\"):\n",
    "            batch_prompts = prompts[i:i+batch_size]\n",
    "            batch_idx = i // batch_size\n",
    "\n",
    "            try:\n",
    "                inputs = tokenizer(batch_prompts, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = model.generate(**inputs, max_new_tokens=150, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "                batch_responses = []\n",
    "                for j, output in enumerate(outputs):\n",
    "                    generated_text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "\n",
    "                    # Clean the response using the cleaning function\n",
    "                    response = clean_response(response)\n",
    "\n",
    "                    # Append cleaned response\n",
    "                    batch_responses.append(response)\n",
    "                \n",
    "                all_responses[i:i+batch_size] = batch_responses  # Assign responses in correct indices\n",
    "\n",
    "                # Save progress to checkpoint\n",
    "                checkpoint = {'responses': all_responses, 'next_batch': batch_idx + 1}\n",
    "                with open(checkpoint_path, 'wb') as f:\n",
    "                    pickle.dump(checkpoint, f)\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {batch_idx}: {e}\")\n",
    "                all_responses[i:i+batch_size] = [\"ERROR_GENERATING_RESPONSE\"] * len(batch_prompts)\n",
    "\n",
    "                checkpoint = {'responses': all_responses, 'next_batch': batch_idx + 1}\n",
    "                with open(checkpoint_path, 'wb') as f:\n",
    "                    pickle.dump(checkpoint, f)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nProcess interrupted. Saving progress...\")\n",
    "        with open(checkpoint_path, 'wb') as f:\n",
    "            pickle.dump({'responses': all_responses, 'next_batch': i // batch_size + 1}, f)\n",
    "\n",
    "    # If any responses are still empty, fill them with error messages\n",
    "    if \"\" in all_responses:\n",
    "        all_responses = [\"ERROR_GENERATING_RESPONSE\" if r == \"\" else r for r in all_responses]  # Fill missing responses\n",
    "\n",
    "    # Remove checkpoint file after processing completes\n",
    "    if len(all_responses) == len(prompts) and os.path.exists(checkpoint_path):\n",
    "        os.remove(checkpoint_path)\n",
    "        print(\"Processing completed, checkpoint removed\")\n",
    "\n",
    "    return all_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (9999, 6) (4998, 6)\n",
      "Test: (1998, 6) (999, 6)\n"
     ]
    }
   ],
   "source": [
    "# Refresher\n",
    "print('Train:', train10k.shape, train5k.shape)\n",
    "print('Test:', test2k.shape, test1k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the chunks\n",
    "chunks = '../data/chunks/'\n",
    "if not os.path.exists(chunks):\n",
    "    os.makedirs(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 <span style=\"font-size:12px;\">Close this for less scrolling...</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train10k into 10 chunks of ~1000 records each\n",
    "train1 = train10k[:1000].copy()\n",
    "train2 = train10k[1000:2000].copy()\n",
    "train3 = train10k[2000:3000].copy()\n",
    "train4 = train10k[3000:4000].copy()\n",
    "train5 = train10k[4000:5000].copy()\n",
    "train6 = train10k[5000:6000].copy()\n",
    "train7 = train10k[6000:7000].copy()\n",
    "train8 = train10k[7000:8000].copy()\n",
    "train9 = train10k[8000:9000].copy()\n",
    "train10 = train10k[9000:].copy()\n",
    "\n",
    "# Split 2k test into 2 chunks of ~1000 records each\n",
    "test1 = test2k[:1000].copy()\n",
    "test2 = test2k[1000:].copy()\n",
    "\n",
    "# Not the most efficient way, but it works..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train 10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0267c6614d6643378fc5232f0fe88f77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed, checkpoint removed\n",
      "e customer's experience and address their concerns, we can take the following steps:\n",
      "\n",
      "1. Acknowledge the issue: Respond to the review by acknowledging the customer's frustration and apologizing for the poor experience they had with our cashier.\n",
      "\n",
      "2. Investigate the incident: Speak with the cashier involved to understand the situation from their perspective and determine if there was a misunderstanding or if the cashier was indeed rude.\n",
      "\n",
      "3. Provide training: Offer additional training to the cashier on customer service skills, focusing on communication, empathy, and problem-solving.\n"
     ]
    }
   ],
   "source": [
    "train1['response'] = generate_response(train1['prompt'], batch_size=50)\n",
    "train1.to_csv(chunks + 'train1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a995b52c243746fea845b02ff660f3d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed, checkpoint removed\n",
      "e customer's experience, we can focus on the following areas:\n",
      "\n",
      "1. Improve Thai Cuisine: The customer mentioned that the Thai food was not up to par. We could consider hiring a Thai chef or providing additional training for our current staff to improve the authenticity and quality of our Thai dishes.\n",
      "\n",
      "2. Address Pricing Concerns: The customer felt that the prices were too high. We could review our pricing strategy and ensure that it is competitive with other establishments in the Bay Area.\n",
      "\n",
      "3. Highlight German Wines: The customer appreciated the selection of German wines.\n"
     ]
    }
   ],
   "source": [
    "train2['response'] = generate_response(train2['prompt'], batch_size=50)\n",
    "train2.to_csv(chunks + 'train2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b55b3b68f6494935a95bcc5b096b6ebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed, checkpoint removed\n",
      "e customer's experience, we can focus on the following areas:\n",
      "\n",
      "1. Improve Drive-Thru Service: Train staff to be more friendly and welcoming, as the customer felt the service was a bit brusque.\n",
      "\n",
      "2. Enhance Food Packaging: Investigate why the hot dogs are coming apart in the wrappers, causing mustard to spill. This could involve changing the type of wrapper or improving the way the food is wrapped.\n",
      "\n",
      "3. Quality Control: Ensure that the hot dogs are properly prepared and not overly wet or juicy, which could contribute to the mustard spilling.\n"
     ]
    }
   ],
   "source": [
    "train3['response'] = generate_response(train3['prompt'], batch_size=50)\n",
    "train3.to_csv(chunks + 'train3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "337af89cfed0426688e76a415fdc76ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed, checkpoint removed\n",
      "e customer's experience, we can focus on enhancing the quality and consistency of our food, specifically the fries, burgers, and sauces. We can aim to make them more flavorful, crispy, and moist, respectively. Additionally, we can work on improving the presentation of our food to make it more appealing.\n",
      "\n",
      "Regarding the customer's comment about the packaging, we can ensure that any religious or personal messages are not visible on the packaging. This will help maintain a neutral and welcoming environment for all customers.\n",
      "\n",
      "Lastly, we can strive to provide exceptional customer service, addressing any concerns or feedback promptly and professionally.\n"
     ]
    }
   ],
   "source": [
    "train4['response'] = generate_response(train4['prompt'], batch_size=50)\n",
    "train4.to_csv(chunks + 'train4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7100841ff9ea4cc2880b95871a7c796d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed, checkpoint removed\n",
      "e customer's experience at your Panda Express location, consider the following steps:\n",
      "\n",
      "1. Freshness: Ensure that food is prepared and served as fresh as possible. This may involve adjusting cooking times or procedures to maintain quality.\n",
      "\n",
      "2. Service: Train staff to provide friendly, efficient, and attentive service. This includes greeting customers promptly, addressing any concerns or questions they may have, and maintaining a clean and organized dining area.\n",
      "\n",
      "3. Cleanliness: Regularly clean and sanitize all areas of the restaurant, including kitchen equipment, dining tables, and restrooms. This will help to maintain a hygienic environment and improve the overall dining experience.\n"
     ]
    }
   ],
   "source": [
    "train5['response'] = generate_response(train5['prompt'], batch_size=50)\n",
    "train5.to_csv(chunks + 'train5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train6[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_response\u001b[49m(train6[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m'\u001b[39m], batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'generate_response' is not defined"
     ]
    }
   ],
   "source": [
    "train6['response'] = generate_response(train6['prompt'], batch_size=50)\n",
    "train6.to_csv(chunks + 'train6.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f434fabe124a410685c4a550a03b2036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed, checkpoint removed\n",
      "6000    e customer's experience, we can take the follo...\n",
      "6001    e customer's experience, we can focus on the f...\n",
      "6002    liability: Ensure timely delivery of orders, e...\n",
      "6003    e customer's experience, we can take the follo...\n",
      "6004    e customer's experience, we can focus on the f...\n",
      "Name: response, dtype: object\n"
     ]
    }
   ],
   "source": [
    "train7['response'] = generate_response(train7['prompt'], batch_size=50)\n",
    "train7.to_csv(chunks + 'train7.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3d3811e2560460cbd59a372bb687301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed, checkpoint removed\n",
      "7000    e customer's experience, we can focus on the f...\n",
      "7001    e customer's experience, we can focus on the f...\n",
      "7002    e customer's experience, we can focus on the f...\n",
      "7003    e customer's experience, we could consider the...\n",
      "7004    e customer's experience at the Tempe location,...\n",
      "Name: response, dtype: object\n"
     ]
    }
   ],
   "source": [
    "train8['response'] = generate_response(train8['prompt'], batch_size=50)\n",
    "train8.to_csv(chunks + 'train8.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b98dccc0ffa24cf08df2ebc9b27a07af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed, checkpoint removed\n",
      "8000    e customer's experience, consider staffing mor...\n",
      "8001    e customer's experience, we can focus on the f...\n",
      "8002    e customer's experience, we can focus on the f...\n",
      "8003    e customer's experience, we could consider the...\n",
      "8004    e customer's experience, consider the followin...\n",
      "Name: response, dtype: object\n"
     ]
    }
   ],
   "source": [
    "train9['response'] = generate_response(train9['prompt'], batch_size=50)\n",
    "train9.to_csv(chunks + 'train9.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "995686d593854626a79877b8596de33a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed, checkpoint removed\n",
      "9000    e customer's experience, we could consider exp...\n",
      "9001    e customer's experience, it would be beneficia...\n",
      "9002    e customer's experience, consider implementing...\n",
      "9003    e customer's experience, we could focus on the...\n",
      "9004    e customer's experience, we can focus on the f...\n",
      "Name: response, dtype: object\n"
     ]
    }
   ],
   "source": [
    "train10['response'] = generate_response(train10['prompt'], batch_size=50)\n",
    "train10.to_csv(chunks + 'train10.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 2k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e5c151720cd4ad19dc910fc506a7aba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed, checkpoint removed\n",
      "e customer's experience, we can focus on the following areas:\n",
      "\n",
      "1. Phone etiquette: Train staff to answer calls professionally, with a friendly and courteous tone. Standard phrases such as \"please,\" \"thank you,\" and \"you're welcome\" should be used consistently.\n",
      "\n",
      "2. Multilingual support: Ensure that staff can communicate effectively with customers who speak languages other than English. Providing clear instructions on how to switch languages or offering bilingual staff can help improve the customer experience.\n",
      "\n",
      "3. Order accuracy and timeliness: Improve the efficiency of the order process to ensure that orders are prepared and ready for pickup as promised.\n"
     ]
    }
   ],
   "source": [
    "test1['response'] = generate_response(test1['prompt'], batch_size=50)\n",
    "test1.to_csv(chunks + 'test1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e customer's experience, we can focus on the following areas:\n",
      "\n",
      "1. Phone etiquette: Train staff to answer calls professionally, with a friendly and courteous tone. Standard phrases such as \"please,\" \"thank you,\" and \"you're welcome\" should be used consistently.\n",
      "\n",
      "2. Multilingual support: Ensure that staff can communicate effectively with customers who speak languages other than English. Providing clear instructions on how to switch languages or offering bilingual staff can help improve the customer experience.\n",
      "\n",
      "3. Order accuracy and timeliness: Improve the efficiency of the order process to ensure that orders are prepared and ready for pickup as promised.\n"
     ]
    }
   ],
   "source": [
    "test2['response'] = generate_response(test2['prompt'], batch_size=50)\n",
    "test2.to_csv(chunks + 'test2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 <span style=\"font-size:12px;\">Close this for less scrolling...</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train5k into 5 chunks of ~1000 records each\n",
    "train11 = train5k[:1000].copy()\n",
    "train12 = train5k[1000:2000].copy()\n",
    "train13 = train5k[2000:3000].copy()\n",
    "train14 = train5k[3000:4000].copy()\n",
    "train15 = train5k[4000:].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train5k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e26e0d32a52145bc958790a749230cca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed, checkpoint removed\n",
      "0    e customer's experience and address their conc...\n",
      "1    e customer's experience, the following steps c...\n",
      "2    e customer's experience, we can focus on the f...\n",
      "3    e customer's experience, we could focus on mod...\n",
      "4    e customer's experience, we can address the fo...\n",
      "Name: response, dtype: object\n"
     ]
    }
   ],
   "source": [
    "train11['response'] = generate_response(train11['prompt'], batch_size=50)\n",
    "train11.to_csv(chunks + 'train11.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5d4308fd5b84035b5d89f8b081dcfa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed, checkpoint removed\n",
      "1000    e customer's experience, we can focus on the f...\n",
      "1001    e taste of the Pad Thai: Enhance the flavor by...\n",
      "1002    e customer's experience, we can take the follo...\n",
      "1003    e customer's experience, we can focus on the f...\n",
      "1004    e customer's experience, we can take the follo...\n",
      "Name: response, dtype: object\n"
     ]
    }
   ],
   "source": [
    "train12['response'] = generate_response(train12['prompt'], batch_size=50)\n",
    "train12.to_csv(chunks + 'train12.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2989ba209cf40389809a4e0e13b0bd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed, checkpoint removed\n",
      "2000    e customer's experience, consider implementing...\n",
      "2001    e customer's experience and potentially win th...\n",
      "2002    e customer's experience and address their conc...\n",
      "2003    e customer's experience and address their conc...\n",
      "2004    e customer's experience, consider the followin...\n",
      "Name: response, dtype: object\n"
     ]
    }
   ],
   "source": [
    "train13['response'] = generate_response(train13['prompt'], batch_size=50)\n",
    "train13.to_csv(chunks + 'train13.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e613d184a443ce815b28d67eea2322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed, checkpoint removed\n",
      "3000    e customer's experience, we can take the follo...\n",
      "3001    e customer's experience, we can focus on the f...\n",
      "3002    e customer's experience, we can focus on the f...\n",
      "3003    e customer's experience, we can take the follo...\n",
      "3004    e customer's experience at your establishment,...\n",
      "Name: response, dtype: object\n"
     ]
    }
   ],
   "source": [
    "train14['response'] = generate_response(train14['prompt'], batch_size=50)\n",
    "train14.to_csv(chunks + 'train14.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed2ee4f2c838417fb5ab28cffcebcfe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed, checkpoint removed\n",
      "4000    e customer's experience, consider the followin...\n",
      "4001    e customer's experience and potentially increa...\n",
      "4002    e customer's experience and potentially increa...\n",
      "4003    e customer's experience, we can focus on the f...\n",
      "4004    e customer's experience, consider the followin...\n",
      "Name: response, dtype: object\n"
     ]
    }
   ],
   "source": [
    "train15['response'] = generate_response(train15['prompt'], batch_size=50)\n",
    "train15.to_csv(chunks + 'train15.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 1k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from checkpoint: 500/999 items processed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5200fd25e9647f8aa15f1ba920bd4ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed, checkpoint removed\n",
      "0    e customer's experience, we can focus on the f...\n",
      "1    e customer's experience, the following steps c...\n",
      "2    e customer's experience, consider implementing...\n",
      "3    e customer's experience, we can take the follo...\n",
      "4    e customer's experience, we can focus on the f...\n",
      "Name: response, dtype: object\n"
     ]
    }
   ],
   "source": [
    "test1k['response'] = generate_response(test1k['prompt'], batch_size=50)\n",
    "test1k.to_csv(chunks + 'test1k.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (1000, 7) (1000, 7) (1000, 7) (1000, 7) (1000, 7) (1000, 7) (1000, 7) (1000, 7) (1000, 7) (999, 7) (1000, 7) (1000, 7) (1000, 7) (1000, 7) (998, 7)\n",
      "Test: (1000, 7) (1000, 7) (999, 7)\n"
     ]
    }
   ],
   "source": [
    "# Load chunks\n",
    "train1 = pd.read_csv(chunks + 'train1.csv')\n",
    "train2 = pd.read_csv(chunks + 'train2.csv')\n",
    "train3 = pd.read_csv(chunks + 'train3.csv')\n",
    "train4 = pd.read_csv(chunks + 'train4.csv')\n",
    "train5 = pd.read_csv(chunks + 'train5.csv')\n",
    "train6 = pd.read_csv(chunks + 'train6.csv')\n",
    "train7 = pd.read_csv(chunks + 'train7.csv')\n",
    "train8 = pd.read_csv(chunks + 'train8.csv')\n",
    "train9 = pd.read_csv(chunks + 'train9.csv')\n",
    "train10 = pd.read_csv(chunks + 'train10.csv')\n",
    "\n",
    "test1 = pd.read_csv(chunks + 'test1.csv')\n",
    "test2 = pd.read_csv(chunks + 'test2.csv')\n",
    "\n",
    "train11 = pd.read_csv(chunks + 'train11.csv')\n",
    "train12 = pd.read_csv(chunks + 'train12.csv')\n",
    "train13 = pd.read_csv(chunks + 'train13.csv')\n",
    "train14 = pd.read_csv(chunks + 'train14.csv')\n",
    "train15 = pd.read_csv(chunks + 'train15.csv')\n",
    "\n",
    "test1k = pd.read_csv(chunks + 'test1k.csv')\n",
    "\n",
    "# Print shapes\n",
    "print('Train:', train1.shape, train2.shape, train3.shape, train4.shape, train5.shape, train6.shape, train7.shape, train8.shape, train9.shape, train10.shape, train11.shape, train12.shape, train13.shape, train14.shape, train15.shape)\n",
    "print('Test:', test1.shape, test2.shape, test1k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the chunks\n",
    "train10k = pd.concat([train1, train2, train3, train4, train5, train6, train7, train8, train9, train10], ignore_index=True)\n",
    "\n",
    "train5k = pd.concat([train11, train12, train13, train14, train15], ignore_index=True)\n",
    "\n",
    "test2k = pd.concat([test1, test2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the directory if it doesn't exist\n",
    "save_here = \"../data/ready_for_phi-2/\"\n",
    "if not os.path.exists(save_here):\n",
    "    os.makedirs(save_here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File train_samples_1.csv saved successfully!\n",
      "File train_samples_2.csv saved successfully!\n",
      "\n",
      "\n",
      "\n",
      "File test_samples_1.csv saved successfully!\n",
      "File test_samples_2.csv saved successfully!\n"
     ]
    }
   ],
   "source": [
    "train_dfs = [train10k, train5k]\n",
    "test_dfs = [test2k, test1k]\n",
    "\n",
    "# Save the splits\n",
    "for i, df in enumerate(train_dfs):\n",
    "    df.to_csv(save_here + f'train_samples_{i+1}.csv', index=False)\n",
    "    if save_here + f'train_0{i+1}.csv':\n",
    "        print(f\"File train_0{i+1}.csv saved successfully!\")\n",
    "\n",
    "print(\"\\n\")\n",
    "        \n",
    "for i, df in enumerate(test_dfs):\n",
    "    df.to_csv(save_here + f'test_0{i+1}.csv', index=False)\n",
    "    if save_here + f'test_0{i+1}.csv':\n",
    "        print(f\"File test_0{i+1}.csv saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
