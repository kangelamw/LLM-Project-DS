{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fine-tuning and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "['eval', 'test_01.csv', 'test_02.csv', 'train_01.csv', 'train_02.csv']\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functions import *\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, Trainer, TrainingArguments\n",
    "import bitsandbytes as bnb\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load data\n",
    "data = '../data/ready_for_phi-2/'\n",
    "\n",
    "print(os.path.exists(data))\n",
    "print(os.listdir(data))\n",
    "\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9999, 3), (4998, 3), (1998, 3), (999, 3))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load\n",
    "train10k = pd.read_csv(data + 'train_01.csv')\n",
    "train5k = pd.read_csv(data + 'train_02.csv')\n",
    "\n",
    "test2k = pd.read_csv(data + 'test_01.csv')\n",
    "test1k = pd.read_csv(data + 'test_02.csv')\n",
    "\n",
    "train10k_benchmark = pd.read_csv(data + 'eval/' + 'train10k_sample.csv')\n",
    "\n",
    "# ===================== #\n",
    "# Selecting only the columns we need for fine-tuning\n",
    "# ===================== #\n",
    "\n",
    "# 15k training samples\n",
    "train10k = train10k[['label', 'prompt', 'response']]\n",
    "train5k = train5k[['label', 'prompt', 'response']]\n",
    "\n",
    "# 3k test samples\n",
    "test2k = test2k[['label', 'prompt', 'response']]\n",
    "test1k = test1k[['label', 'prompt', 'response']]\n",
    "\n",
    "# Rename columns\n",
    "train10k.columns = ['label', 'input', 'mistral_output']\n",
    "train5k.columns = ['label', 'input', 'mistral_output']\n",
    "test2k.columns = ['label', 'input', 'mistral_output']\n",
    "test1k.columns = ['label', 'input', 'mistral_output']\n",
    "\n",
    "train10k.shape, train5k.shape, test2k.shape, test1k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['label', 'input', 'mistral_output'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train10k.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [train10k, train5k, test2k, test1k]\n",
    "\n",
    "for df in dfs:\n",
    "  df['mistral_output'] = df.apply(lambda row: clean_output(row['input'], row['mistral_output']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dfs:\n",
    "  df['input'] = df['input'].str.replace('[INST]', '')\n",
    "  df['input'] = df['input'].str.replace('[/INST]', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "label",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "input",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "mistral_output",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "19e4222e-2afe-4cec-9c5e-320c2a46f29e",
       "rows": [
        [
         "0",
         "2",
         " A customer left us a 2-star review: 'the cashier told my husband that should he get change and the air machine just don't work don't come back in asking for a refund. this location the employees are uneducated on any kind of hospitality when having contact with human customers. i just don't care for employees who have a rude attitude. how do store owners proudly have a rude person as a cashier? well let's see how that will work out for them. attention all friends the people are rude at this location.' The customer feels annoyance, anger, neutral. Concisely, how can we best improve our services for this customer's experience? ",
         "A customer left us a 2-star review: 'the cashier told my husband that should he get change and the air machine just don't work don't come back in asking for a refund. this location the employees are uneducated on any kind of hospitality when having contact with human customers. i just don't care for employees who have a rude attitude. how do store owners proudly have a rude person as a cashier? well let's see how that will work out for them. attention all friends the people are rude at this location.' The customer feels annoyance, anger, neutral. Concisely, how can we best improve our services for this customer's experience? To improve the customer's experience and address their concerns, we can take the following steps: 1. Acknowledge the issue: Respond to the review by acknowledging the customer's frustration and apologizing for the poor experience they had with our cashier. 2. Investigate the incident: Speak with the cashier involved to understand the situation from their perspective and determine if there was a misunderstanding or if the cashier was indeed rude. 3. Provide training: Offer additional training to the cashier on customer service skills, focusing on communication, empathy, and problem-solving."
        ],
        [
         "1",
         "2",
         " A customer left us a 2-star review: 'here's what happened n we were suppose to be at the regular mgm.. so we pulled up, put our car in valet, got a bell boy to take our stuff.. and then we realized they gave us rooms at the signature. what a pain!!!! n then when we tried to check in we were suppose to have 2 rooms... but they couldn't find our other reservation because we booked through an online travel company so they only gave us one room!!! n the next day we had to spend at least an hour trying to get compensated for the room that we pre paid for! finally they gave us back the money and also said they would give us 200 to spend on food and bev. n so, we ordered 158 in food that night... and tipped 30! n then when i went to check out... they had all the room service charges and all the room charges on my card!!! it was such a hassle to get them off!!! in the end, they took off all charges but the 30 tip which i was still mad about . n nthe rooms were super nice! they had a nice kitchen and a nice view. but, i do not consider it much of an upgrade from the mgm since one of the beds is a pull out couch. n nalso, we were there for a bachelorette party... and the bachelorette likes to gamble.. so it probably would have been more ideal to stay in the casino. n noverall, the condo was very nice... but the customer service was totally disorganized and bad. numerous times the signature blamed their poor service on the mgm and said they couldn't do anything about our problems since we booked through the mgm.' The customer feels realization, neutral, disappointment. Concisely, how can we best improve our services for this customer's experience? ",
         "A customer left us a 2-star review: 'here's what happened n we were suppose to be at the regular mgm.. so we pulled up, put our car in valet, got a bell boy to take our stuff.. and then we realized they gave us rooms at the signature. what a pain!!!! n then when we tried to check in we were suppose to have 2 rooms... but they couldn't find our other reservation because we booked through an online travel company so they only gave us one room!!! n the next day we had to spend at least an hour trying to get compensated for the room that we pre paid for! finally they gave us back the money and also said they would give us 200 to spend on food and bev. n so, we ordered 158 in food that night... and tipped 30! n then when i went to check out... they had all the room service charges and all the room charges on my card!!! it was such a hassle to get them off!!! in the end, they took off all charges but the 30 tip which i was still mad about . n nthe rooms were super nice! they had a nice kitchen and a nice view. but, i do not consider it much of an upgrade from the mgm since one of the beds is a pull out couch. n nalso, we were there for a bachelorette party... and the bachelorette likes to gamble.. so it probably would have been more ideal to stay in the casino. n noverall, the condo was very nice... but the customer service was totally disorganized and bad. numerous times the signature blamed their poor service on the mgm and said they couldn't do anything about our problems since we booked through the mgm.' The customer feels realization, neutral, disappointment. Concisely, how can we best improve our services for this customer's experience? To improve the customer's experience, the following steps could be taken: 1. Clear Communication: Ensure that the hotel staff clearly communicates the location of the rooms to the guests, especially when there is a change in the booking. 2. Double-Check Reservations: Double-check all reservations, especially those made through online travel companies, to avoid any confusion or missing reservations. 3. Streamline Check-in Process: Improve the check-in process to ensure that all reservations are accounted for and any issues are resolved promptly. 4. Compensation: When issues arise, offer prompt and fair compensation to the customers."
        ],
        [
         "2",
         "2",
         " A customer left us a 2-star review: 'went to this fest today and admission is 12. there were 2 tiny stages where live entertainment was scheduled. there was a food truck, a catered brazilian meal stand and some lebanese food ??? . the booths were boring and mainly were hawking insurance or travel or sports. there weren't any booths offering anything to do with brazil. there were streamers in yellow and green and that was the extent of the decorations. the schedules were preprinted which was good...but it's not like there was anything to do other than eat, drink or watch the stage. and it was very hot outside. the portion that was supposed to be 8 of brazilian food was small and cold. and there was only one line to order and one line to pick up. seems like if this is the 5th annual event, there would be more attention to details. it was even hot inside the building and i really don't know what jui jitsu has to do with brazil. that country has such a rich culture, history and importance to the ecosystems of the world and all that was wasted at this festival. the good? public parking was free.' The customer feels neutral, disapproval, approval. Concisely, how can we best improve our services for this customer's experience? ",
         "A customer left us a 2-star review: 'went to this fest today and admission is 12. there were 2 tiny stages where live entertainment was scheduled. there was a food truck, a catered brazilian meal stand and some lebanese food ??? . the booths were boring and mainly were hawking insurance or travel or sports. there weren't any booths offering anything to do with brazil. there were streamers in yellow and green and that was the extent of the decorations. the schedules were preprinted which was good...but it's not like there was anything to do other than eat, drink or watch the stage. and it was very hot outside. the portion that was supposed to be 8 of brazilian food was small and cold. and there was only one line to order and one line to pick up. seems like if this is the 5th annual event, there would be more attention to details. it was even hot inside the building and i really don't know what jui jitsu has to do with brazil. that country has such a rich culture, history and importance to the ecosystems of the world and all that was wasted at this festival. the good? public parking was free.' The customer feels neutral, disapproval, approval. Concisely, how can we best improve our services for this customer's experience? To improve the customer's experience, we can focus on the following areas: 1. Expand and diversify entertainment: Offer a wider variety of live entertainment that better represents Brazil's rich culture, such as music, dance, and art performances. 2. Improve food offerings: Offer more substantial and hot Brazilian food options, with multiple lines for ordering and picking up to reduce wait times. 3. Enhance decorations: Increase the use of decorations that reflect Brazil's culture, history, and importance to the ecosystems of the world."
        ],
        [
         "3",
         "2",
         " A customer left us a 2-star review: 'went to check out this joint the other weekend when i was in vegas. its certainly off the beaten path or strip for that matter... nwas it worth the extra travel time to get to the strip...that remains to be seen. the place looks nice but it has an old casino type of feel...and not in a good sense. nso i may be back in vegas before year is out and if time permits...try to hit the golf course. nwe will see.....' The customer feels admiration, approval, neutral. Concisely, how can we best improve our services for this customer's experience? ",
         "A customer left us a 2-star review: 'went to check out this joint the other weekend when i was in vegas. its certainly off the beaten path or strip for that matter... nwas it worth the extra travel time to get to the strip...that remains to be seen. the place looks nice but it has an old casino type of feel...and not in a good sense. nso i may be back in vegas before year is out and if time permits...try to hit the golf course. nwe will see.....' The customer feels admiration, approval, neutral. Concisely, how can we best improve our services for this customer's experience? To improve the customer's experience, we could focus on modernizing the ambiance of our establishment to make it less reminiscent of an old casino. This could involve updating the decor, improving the lighting, and potentially enhancing the overall cleanliness and maintenance of the venue. Additionally, we could consider offering incentives or promotions to make the extra travel time to reach our location more appealing, such as discounts on food, drinks, or activities. Lastly, ensuring excellent customer service and a welcoming atmosphere could help create a more positive impression and encourage repeat visits."
        ],
        [
         "4",
         "2",
         " A customer left us a 2-star review: 'a whole lot of meh. this store is in a horrible location and the store hasn't received the corporate refresh yet like the store down the road. beyond that it's a dang home depot' The customer feels disappointment, sadness, fear. Concisely, how can we best improve our services for this customer's experience? ",
         "A customer left us a 2-star review: 'a whole lot of meh. this store is in a horrible location and the store hasn't received the corporate refresh yet like the store down the road. beyond that it's a dang home depot' The customer feels disappointment, sadness, fear. Concisely, how can we best improve our services for this customer's experience? To improve the customer's experience, we can address the following concerns: 1. Location: Although we cannot change the physical location of the store, we can offer delivery services to make shopping more convenient for customers. We could also consider offering promotions or discounts for in-store pickup to encourage customers to visit our store. 2. Corporate refresh: We can expedite the store refresh to ensure that it matches the corporate standards and provides a more appealing shopping environment. 3. Competition with Home Depot: To differentiate ourselves from Home Depot, we can focus on offering unique products, exceptional customer service, and personalized shopping experiences that cater to our customers' specific needs."
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>input</th>\n",
       "      <th>mistral_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>A customer left us a 2-star review: 'the cash...</td>\n",
       "      <td>A customer left us a 2-star review: 'the cashi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A customer left us a 2-star review: 'here's w...</td>\n",
       "      <td>A customer left us a 2-star review: 'here's wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>A customer left us a 2-star review: 'went to ...</td>\n",
       "      <td>A customer left us a 2-star review: 'went to t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>A customer left us a 2-star review: 'went to ...</td>\n",
       "      <td>A customer left us a 2-star review: 'went to c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>A customer left us a 2-star review: 'a whole ...</td>\n",
       "      <td>A customer left us a 2-star review: 'a whole l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              input  \\\n",
       "0      2   A customer left us a 2-star review: 'the cash...   \n",
       "1      2   A customer left us a 2-star review: 'here's w...   \n",
       "2      2   A customer left us a 2-star review: 'went to ...   \n",
       "3      2   A customer left us a 2-star review: 'went to ...   \n",
       "4      2   A customer left us a 2-star review: 'a whole ...   \n",
       "\n",
       "                                      mistral_output  \n",
       "0  A customer left us a 2-star review: 'the cashi...  \n",
       "1  A customer left us a 2-star review: 'here's wh...  \n",
       "2  A customer left us a 2-star review: 'went to t...  \n",
       "3  A customer left us a 2-star review: 'went to c...  \n",
       "4  A customer left us a 2-star review: 'a whole l...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train10k.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving\n",
    "fine_tuning_data = '../data/for_step_4/'\n",
    "\n",
    "if not os.path.exists(fine_tuning_data):\n",
    "  os.makedirs(fine_tuning_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 1 saved successfully!\n",
      "File 2 saved successfully!\n",
      "File 3 saved successfully!\n",
      "File 4 saved successfully!\n"
     ]
    }
   ],
   "source": [
    "train10k.to_csv(fine_tuning_data + 'train10k.csv', index=False)\n",
    "train5k.to_csv(fine_tuning_data + 'train5k.csv', index=False)\n",
    "test2k.to_csv(fine_tuning_data + 'test2k.csv', index=False)\n",
    "test1k.to_csv(fine_tuning_data + 'test1k.csv', index=False)\n",
    "\n",
    "if fine_tuning_data + 'train10k.csv':\n",
    "  print('File 1 saved successfully!')\n",
    "if fine_tuning_data + 'train5k.csv':\n",
    "  print('File 2 saved successfully!')\n",
    "if fine_tuning_data + 'test2k.csv':\n",
    "  print('File 3 saved successfully!')\n",
    "if fine_tuning_data + 'test1k.csv':\n",
    "  print('File 4 saved successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ec4a4c64e2646b98df5fcbcec75570a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "! Model loaded on: cuda\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): PhiForCausalLM(\n",
       "      (model): PhiModel(\n",
       "        (embed_tokens): Embedding(51200, 2560)\n",
       "        (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x PhiDecoderLayer(\n",
       "            (self_attn): PhiSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (dense): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
       "              (rotary_emb): PhiRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): PhiMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=10240, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=10240, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (fc2): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=10240, out_features=2560, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=10240, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (rotary_emb): PhiRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Define 4-bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"microsoft/phi-2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             quantization_config=bnb_config,\n",
    "                                             device_map={\"\": 0})\n",
    "\n",
    "# Prepare model for kbit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    # Target the attention modules in Phi-2\n",
    "    target_modules=[\"q_proj\", \n",
    "                    \"k_proj\", \n",
    "                    \"v_proj\", \n",
    "                    \"out_proj\",\n",
    "                    \"fc1\",\n",
    "                    \"fc2\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Get model device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"==========\\n! Model loaded on: {device}\\n\")\n",
    "# print(model.quantization_method)\n",
    "# print(model.config)\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory allocated: 2392.86 MB\n",
      "GPU memory cached: 2472.54 MB\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Print GPU memory usage\n",
    "print(f\"GPU memory allocated: {torch.cuda.memory_allocated()/1e6:.2f} MB\")\n",
    "print(f\"GPU memory cached: {torch.cuda.memory_reserved()/1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, _ in model.named_modules():\n",
    "#   print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "  tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tokenize_function(df):\n",
    "    combined = []\n",
    "    for i in range(len(df[\"input\"])):\n",
    "        combined.append(\n",
    "            \"Instruction: \" + df[\"input\"][i] + \"\\nResponse: \" + df[\"mistral_output\"][i]\n",
    "        )\n",
    "    \n",
    "    # Tokenize sequences\n",
    "    tokenized = tokenizer(\n",
    "        combined,\n",
    "        max_length=2048,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    labels = tokenized[\"input_ids\"].clone()\n",
    "   \n",
    "    # Mask out the loss for the instruction part\n",
    "    for i in range(len(combined)):\n",
    "        instruction = \"Instruction: \" + df[\"input\"][i] + \"\\nResponse: \"\n",
    "        instruction_tokens = tokenizer(instruction, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        instruction_length = len(instruction_tokens)\n",
    "       \n",
    "        # Set labels to -100 for the instruction part (no loss computed)\n",
    "        labels[i, :instruction_length] = -100\n",
    "   \n",
    "    return {\n",
    "        \"input_ids\": tokenized[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized[\"attention_mask\"],\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tokenize_function(df):\n",
    "    # Tokenize the prompt (input)\n",
    "    tokenized_inputs = tokenizer(\n",
    "        df[\"input\"],\n",
    "        max_length=2048,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Tokenize the gold responses (mistral_output)\n",
    "    tokenized_gold = tokenizer(\n",
    "        df[\"mistral_output\"],\n",
    "        max_length=2048,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Use the gold tokenized outputs as labels for metric computation\n",
    "    tokenized_inputs[\"labels\"] = tokenized_gold[\"input_ids\"]\n",
    "    \n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a9e2960b7e8422c9626bf4220b0b4ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/999 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b7b35af1c244173ad538f049347c871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/333 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Saving\n",
    "fine_tuning_data = '../data/for_step_4/'\n",
    "\n",
    "# Load\n",
    "train10k = pd.read_csv(fine_tuning_data + 'train10k.csv')\n",
    "#train5k = pd.read_csv(fine_tuning_data + 'train5k.csv')\n",
    "test2k = pd.read_csv(fine_tuning_data + 'test2k.csv')\n",
    "#test1k = pd.read_csv(fine_tuning_data + 'test1k.csv')\n",
    "\n",
    "\n",
    "# Time constraints. I'll just use 999 train, 333 test samples\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_left, train = train_test_split(train10k, \n",
    "                         test_size=999, \n",
    "                         random_state=42,\n",
    "                         stratify=train10k['label'])\n",
    "train_left, test = train_test_split(test2k, \n",
    "                        test_size=333, \n",
    "                        random_state=42,\n",
    "                        stratify=test2k['label'])\n",
    "\n",
    "# train.shape, test.shape, train['label'].value_counts(), test['label'].value_counts()\n",
    "\n",
    "# Selecting only the columns we need for fine-tuning, I was splitting by label.\n",
    "train = train[['input', 'mistral_output']]\n",
    "test = test[['input', 'mistral_output']]\n",
    "\n",
    "# Convert to HuggingFace Datasets\n",
    "train_dataset = Dataset.from_pandas(train)\n",
    "train_dataset = train_dataset.remove_columns(['__index_level_0__'])\n",
    "\n",
    "test_dataset = Dataset.from_pandas(test)\n",
    "test_dataset = test_dataset.remove_columns(['__index_level_0__'])\n",
    "\n",
    "# Tokenize\n",
    "tokenized_train_dataset = train_dataset.map(train_tokenize_function,\n",
    "                                            batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(test_tokenize_function,\n",
    "                                          batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Selecting only the columns we need for fine-tuning, I was splitting by label.\\ntrain10k = train10k[['input', 'mistral_output']]\\ntrain5k = train5k[['input', 'mistral_output']]\\ntest2k = test2k[['input']]\\ntest1k = test1k[['input']]\\n\\n# Convert to HuggingFace Datasets\\nfrom datasets import Dataset\\n\\ntrain = Dataset.from_pandas(train)\\ntest = Dataset.from_pandas(test)\\n\\ntrain10k_dataset = Dataset.from_pandas(train10k)\\ntrain5k_dataset = Dataset.from_pandas(train5k)\\ntest2k_dataset = Dataset.from_pandas(test2k)\\ntest1k_dataset = Dataset.from_pandas(test1k)\\n\\n# Tokenize\\ntrain = train.map(train_tokenize_function,\\n                  batched=True)\\ntest = test.map(test_tokenize_function, \\n                  batched=True)\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Selecting only the columns we need for fine-tuning, I was splitting by label.\n",
    "train10k = train10k[['input', 'mistral_output']]\n",
    "train5k = train5k[['input', 'mistral_output']]\n",
    "test2k = test2k[['input']]\n",
    "test1k = test1k[['input']]\n",
    "\n",
    "# Convert to HuggingFace Datasets\n",
    "from datasets import Dataset\n",
    "\n",
    "train = Dataset.from_pandas(train)\n",
    "test = Dataset.from_pandas(test)\n",
    "\n",
    "train10k_dataset = Dataset.from_pandas(train10k)\n",
    "train5k_dataset = Dataset.from_pandas(train5k)\n",
    "test2k_dataset = Dataset.from_pandas(test2k)\n",
    "test1k_dataset = Dataset.from_pandas(test1k)\n",
    "\n",
    "# Tokenize\n",
    "train = train.map(train_tokenize_function,\n",
    "                  batched=True)\n",
    "test = test.map(test_tokenize_function, \n",
    "                  batched=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_train_dataset, tokenized_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tokenizer.decode(tokenized_train_dataset['input_ids'][0]))\n",
    "#print('\\n\\n')\n",
    "\n",
    "#print(tokenizer.decode(tokenized_test_dataset['input_ids'][0]))\n",
    "#print(tokenizer.decode(tokenized_test_dataset['labels'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model output\n",
    "model_path = '../models/'\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "  os.makedirs(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default BLEURT-Base checkpoint for sequence maximum length 128. You can use a bigger model for better results with e.g.: evaluate.load('bleurt', 'bleurt-large-512').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint C:\\Users\\kadm2\\.cache\\huggingface\\metrics\\bleurt\\default\\downloads\\extracted\\1ed47b7280a9b4162e745e8e509d21a5ca48976269d7d45c0c7b6c6e350c760e\\bleurt-base-128.\n",
      "INFO:tensorflow:Config file found, reading.\n",
      "INFO:tensorflow:Will load checkpoint bert_custom\n",
      "INFO:tensorflow:Loads full paths and checks that files exists.\n",
      "INFO:tensorflow:... name:bert_custom\n",
      "INFO:tensorflow:... vocab_file:vocab.txt\n",
      "INFO:tensorflow:... bert_config_file:bert_config.json\n",
      "INFO:tensorflow:... do_lower_case:True\n",
      "INFO:tensorflow:... max_seq_length:128\n",
      "INFO:tensorflow:Creating BLEURT scorer.\n",
      "INFO:tensorflow:Creating WordPiece tokenizer.\n",
      "INFO:tensorflow:WordPiece tokenizer instantiated.\n",
      "INFO:tensorflow:Creating Eager Mode predictor.\n",
      "INFO:tensorflow:Loading model.\n",
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kadm2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\kadm2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\kadm2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import evaluate\n",
    "\n",
    "# Load evaluation metrics\n",
    "bleurt = evaluate.load(\"bleurt\", trust_remote_code=True)\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "meteor = evaluate.load(\"meteor\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    \"\"\"\n",
    "    Computes BLEURT, BERTScore (F1), and METEOR for evaluating generated feedback.\n",
    "\n",
    "    Args:\n",
    "        eval_preds (tuple): A tuple containing (predictions, references).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with metric scores.\n",
    "    \"\"\"\n",
    "    predictions, references = eval_preds\n",
    "\n",
    "    # Ensure string format\n",
    "    predictions = [str(pred).strip() for pred in predictions]\n",
    "    references = [str(ref).strip() for ref in references]\n",
    "\n",
    "    # Compute BLEURT\n",
    "    bleurt_scores = bleurt.compute(predictions=predictions, references=references)[\"scores\"]\n",
    "\n",
    "    # Compute BERTScore(F1)\n",
    "    bertscore_f1 = bertscore.compute(predictions=predictions, references=references, lang=\"en\")[\"f1\"]\n",
    "\n",
    "    # Compute METEOR\n",
    "    meteor_score = meteor.compute(predictions=predictions, references=references)[\"meteor\"]\n",
    "\n",
    "    metrics = {\n",
    "        \"bleurt\": np.mean(bleurt_scores),\n",
    "        \"bertscore_f1\": np.mean(bertscore_f1),\n",
    "        \"meteor\": meteor_score\n",
    "    }\n",
    "\n",
    "    logger.info(f\"Eval Metrics: {metrics}\")  # Force logging\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "  output_dir=model_path,\n",
    "  do_train=True,\n",
    "  do_eval=True,\n",
    "  do_predict=True,\n",
    "  \n",
    "  eval_strategy=\"epoch\",\n",
    "  save_strategy=\"epoch\",\n",
    "  eval_accumulation_steps=8,\n",
    "  logging_strategy=\"epoch\",\n",
    "  num_train_epochs=3,\n",
    "  \n",
    "  learning_rate=2e-5,\n",
    "  lr_scheduler_type=\"cosine\",\n",
    "  warmup_ratio=0.1,\n",
    "  weight_decay=0.01,\n",
    "  \n",
    "  fp16=True,\n",
    "  dataloader_num_workers=4,\n",
    "  \n",
    "  optim=\"adamw_torch_fused\",\n",
    "  load_best_model_at_end=True,\n",
    "  metric_for_best_model=\"meteor\",\n",
    "  greater_is_better=True,\n",
    "  \n",
    "  report_to=\"tensorboard\",\n",
    "  resume_from_checkpoint=True,\n",
    "  \n",
    "  per_device_eval_batch_size=2,\n",
    "  per_device_train_batch_size=2,\n",
    "  gradient_accumulation_steps=2,\n",
    "  gradient_checkpointing=True,\n",
    "  save_total_limit=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(os.path.join(model_path, 'phi-2_01'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2746aa07db25488aa7564493b44d269f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PhiForCausalLM(\n",
       "  (model): PhiModel(\n",
       "    (embed_tokens): Embedding(51200, 2560)\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x PhiDecoderLayer(\n",
       "        (self_attn): PhiSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (dense): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rotary_emb): PhiRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): PhiMLP(\n",
       "          (activation_fn): NewGELUActivation()\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "    (rotary_emb): PhiRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "base_model_name = \"microsoft/phi-2\"  # Your original model\n",
    "adapter_path = \"../models/phi-2_01\"\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "# Load LoRA adapter and merge\n",
    "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "model = model.merge_and_unload()  # Merge LoRA with base model\n",
    "\n",
    "# Move to GPU if available\n",
    "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model_path = \"../models/phi-2_full\"\n",
    "#model.save_pretrained(merged_model_path)\n",
    "#tokenizer.save_pretrained(merged_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03d2041ced744ec3908775371b406393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PhiForCausalLM(\n",
       "  (model): PhiModel(\n",
       "    (embed_tokens): Embedding(51200, 2560)\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x PhiDecoderLayer(\n",
       "        (self_attn): PhiSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (dense): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rotary_emb): PhiRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): PhiMLP(\n",
       "          (activation_fn): NewGELUActivation()\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "    (rotary_emb): PhiRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(merged_model_path)\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    merged_model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 2779683840\n",
      "Total parameters: 2779683840\n"
     ]
    }
   ],
   "source": [
    "# Check for trainable parameters in the model\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"Trainable parameters: {trainable_params}\")\n",
    "print(f\"Total parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[INST] A customer left us a 2-star review: 'the cashier told my husband that should he get change and the air machine just don't work don't come back in asking for a refund. this location the employees are uneducated on any kind of hospitality when having contact with human customers. i just don't care for employees who have a rude attitude. how do store owners proudly have a rude person as a cashier? well let's see how that will work out for them. attention all friends the people are rude at this location.' The customer feels annoyance, anger, neutral. Concisely, how can we best improve our services for this customer's experience? [/INST]\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train10k['input'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] A customer left us a 2-star review: 'here's what happened n we were suppose to be at the regular mgm.. so we pulled up, put our car in valet, got a bell boy to take our stuff.. and then we realized they gave us rooms at the signature. what a pain!!!! n then when we tried to check in we were suppose to have 2 rooms... but they couldn't find our other reservation because we booked through an online travel company so they only gave us one room!!! n the next day we had to spend at least an hour trying to get compensated for the room that we pre paid for! finally they gave us back the money and also said they would give us 200 to spend on food and bev. n so, we ordered 158 in food that night... and tipped 30! n then when i went to check out... they had all the room service charges and all the room charges on my card!!! it was such a hassle to get them off!!! in the end, they took off all charges but the 30 tip which i was still mad about. n nthe rooms were super nice! they had a nice kitchen and a nice view. but, i do not consider it much of an upgrade from the mgm since one of the beds is a pull out couch. n nalso, we were there for a bachelorette party... and the bachelorette likes to gamble.. so it probably would have been more ideal to stay in the casino. n noverall, the condo was very nice... but the customer service was totally disorganized and bad. numerous times the signature blamed their poor service on the mgm and said they couldn't do anything about our problems since we booked through the mgm.' The customer feels realization, neutral, disappointment. Concisely, how can we best improve our services for this customer's experience? [/INST]\n"
     ]
    }
   ],
   "source": [
    "prompt = train10k['input'][1]\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():  # No need to track gradients during inference\n",
    "    output = model.generate(**inputs, \n",
    "                            max_new_tokens=500,\n",
    "                            no_repeat_ngram_size=3,\n",
    "                            do_sample=True,\n",
    "                            pad_token_id=tokenizer.eos_token_id,\n",
    "                            temperature=0.2,\n",
    "                            top_p=0.9,\n",
    "                            top_k=50) \n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))  # Decode and print the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58, 38604, 60, 317, 6491, 1364, 514, 257, 362, 12, 7364, 2423, 25, 705, 1456, 338, 644, 3022, 299, 356, 547, 11691, 284, 307, 379, 262, 3218, 10527, 76, 492, 523, 356, 5954, 510, 11, 1234, 674, 1097, 287, 1188, 316, 11, 1392, 257, 8966, 2933, 284, 1011, 674, 3404, 492, 290, 788, 356, 6939, 484, 2921, 514, 9519, 379, 262, 9877, 13, 644, 257, 2356, 13896, 299, 788, 618, 356, 3088, 284, 2198, 287, 356, 547, 11691, 284, 423, 362, 9519, 986, 475, 484, 3521, 470, 1064, 674, 584, 24048, 780, 356, 21765, 832, 281, 2691, 3067, 1664, 523, 484, 691, 2921, 514, 530, 2119, 10185, 299, 262, 1306, 1110, 356, 550, 284, 4341, 379, 1551, 281, 1711, 2111, 284, 651, 34304, 329, 262, 2119, 326, 356, 662, 3432, 329, 0, 3443, 484, 2921, 514, 736, 262, 1637, 290, 635, 531, 484, 561, 1577, 514, 939, 284, 4341, 319, 2057, 290, 307, 85, 13, 299, 523, 11, 356, 6149, 24063, 287, 2057, 326, 1755, 986, 290, 28395, 1542, 0, 299, 788, 618, 1312, 1816, 284, 2198, 503, 986, 484, 550, 477, 262, 2119, 2139, 4530, 290, 477, 262, 2119, 4530, 319, 616, 2657, 10185, 340, 373, 884, 257, 32721, 284, 651, 606, 572, 10185, 287, 262, 886, 11, 484, 1718, 572, 477, 4530, 475, 262, 1542, 8171, 543, 1312, 373, 991, 8805, 546, 764, 299, 299, 1169, 9519, 547, 2208, 3621, 0, 484, 550, 257, 3621, 9592, 290, 257, 3621, 1570, 13, 475, 11, 1312, 466, 407, 2074, 340, 881, 286, 281, 8515, 422, 262, 10527, 76, 1201, 530, 286, 262, 20237, 318, 257, 2834, 503, 18507, 13, 299, 299, 14508, 11, 356, 547, 612, 329, 257, 275, 9636, 9997, 660, 2151, 986, 290, 262, 275, 9636, 9997, 660, 7832, 284, 32385, 492, 523, 340, 2192, 561, 423, 587, 517, 7306, 284, 2652, 287, 262, 21507, 13, 299, 645, 332, 439, 11, 262, 32119, 373, 845, 3621, 986, 475, 262, 6491, 2139, 373, 6635, 595, 30280, 290, 2089, 13, 6409, 1661, 262, 9877, 13772, 511, 3595, 2139, 319, 262, 10527, 76, 290, 531, 484, 3521, 470, 466, 1997, 546, 674, 2761, 1201, 356, 21765, 832, 262, 10527, 76, 2637, 383, 6491, 5300, 23258, 11, 8500, 11, 18641, 13, 13223, 786, 306, 11, 703, 460, 356, 1266, 2987, 674, 2594, 329, 428, 6491, 338, 1998, 30, 46581, 38604, 60]\n",
      "[INST] A customer left us a 2-star review: 'here's what happened n we were suppose to be at the regular mgm.. so we pulled up, put our car in valet, got a bell boy to take our stuff.. and then we realized they gave us rooms at the signature. what a pain!!!! n then when we tried to check in we were suppose to have 2 rooms... but they couldn't find our other reservation because we booked through an online travel company so they only gave us one room!!! n the next day we had to spend at least an hour trying to get compensated for the room that we pre paid for! finally they gave us back the money and also said they would give us 200 to spend on food and bev. n so, we ordered 158 in food that night... and tipped 30! n then when i went to check out... they had all the room service charges and all the room charges on my card!!! it was such a hassle to get them off!!! in the end, they took off all charges but the 30 tip which i was still mad about. n nthe rooms were super nice! they had a nice kitchen and a nice view. but, i do not consider it much of an upgrade from the mgm since one of the beds is a pull out couch. n nalso, we were there for a bachelorette party... and the bachelorette likes to gamble.. so it probably would have been more ideal to stay in the casino. n noverall, the condo was very nice... but the customer service was totally disorganized and bad. numerous times the signature blamed their poor service on the mgm and said they couldn't do anything about our problems since we booked through the mgm.' The customer feels realization, neutral, disappointment. Concisely, how can we best improve our services for this customer's experience? [/INST]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(prompt))\n",
    "print(tokenizer.decode(tokenizer.encode(prompt)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
